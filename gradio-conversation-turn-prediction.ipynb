{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of GPTNeoForTokenClassification were not initialized from the model checkpoint at EleutherAI/gpt-neo-125m and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/tmp/ipykernel_28066/3626123833.py:17: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(model_weights_path, map_location=DEVICE))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GPTNeoForTokenClassification(\n",
       "  (transformer): GPTNeoModel(\n",
       "    (wte): Embedding(50257, 768)\n",
       "    (wpe): Embedding(2048, 768)\n",
       "    (drop): Dropout(p=0.0, inplace=False)\n",
       "    (h): ModuleList(\n",
       "      (0-11): 12 x GPTNeoBlock(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPTNeoAttention(\n",
       "          (attention): GPTNeoSelfAttention(\n",
       "            (attn_dropout): Dropout(p=0.0, inplace=False)\n",
       "            (resid_dropout): Dropout(p=0.0, inplace=False)\n",
       "            (k_proj): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (v_proj): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (q_proj): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPTNeoMLP(\n",
       "          (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=768, out_features=5, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gradio as gr\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForTokenClassification \n",
    "import torch.nn.functional as F\n",
    "import pandas as pd\n",
    "\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Load the pretrained tokenizer and our trained model\n",
    "model_checkpoint = \"EleutherAI/gpt-neo-125m\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint, add_prefix_space=True)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# Load the trained weights\n",
    "model = AutoModelForTokenClassification.from_pretrained(model_checkpoint, num_labels=5)\n",
    "model_weights_path = \"model_weights/conversation_classifier.pth\"\n",
    "model.load_state_dict(torch.load(model_weights_path, map_location=DEVICE))\n",
    "model.to(DEVICE)  # Move the model to the appropriate device\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7870\n",
      "* Running on public URL: https://74ffdb736a8b26cec4.gradio.live\n",
      "\n",
      "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"https://74ffdb736a8b26cec4.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "MAX_SEQ_LENGTH = 512\n",
    "\n",
    "# These weights are the inverse of the class frequencies in the training data\n",
    "# Using to try to correct for the imbalance in the training data\n",
    "def calculate_class_weights():\n",
    "    # Calculate class weights\n",
    "    class_frequencies = {\n",
    "        0: 0.2546,  # Very Negative\n",
    "        1: 0.3903,  # Negative\n",
    "        2: 0.2477,  # Neutral\n",
    "        3: 0.0456,  # Positive\n",
    "        4: 0.0620   # Very Positive\n",
    "    }\n",
    "    # Calculate inverse of frequencies\n",
    "    inverse_frequencies = {class_id: 1/freq for class_id, freq in class_frequencies.items()}\n",
    "    \n",
    "    # Normalize weights so they sum to 1\n",
    "    total = sum(inverse_frequencies.values())\n",
    "    normalized_weights = {class_id: inv_freq/total for class_id, inv_freq in inverse_frequencies.items()}\n",
    "    \n",
    "    # Convert to tensor\n",
    "    class_weights = torch.tensor([normalized_weights[i] for i in range(len(class_frequencies))]).float()\n",
    "    \n",
    "    return class_weights\n",
    "\n",
    "CLASS_WEIGHTS = calculate_class_weights().to(DEVICE)\n",
    "\n",
    "\n",
    "def create_confidence_histogram(probabilities):\n",
    "    labels = [\"Very Negative\", \"Negative\", \"Neutral\", \"Positive\", \"Very Positive\"]\n",
    "    colors = ['red', 'orange', 'gray', 'lightblue', 'blue']\n",
    "    \n",
    "    # Get the probabilities for the last token\n",
    "    last_token_probs = probabilities[0, -1, :].cpu().numpy()\n",
    "    \n",
    "    histogram_html = \"<div style='display: flex; align-items: center; font-size: 12px; margin-bottom: 5px;'>\"\n",
    "    for i, (label, prob, color) in enumerate(zip(labels, last_token_probs, colors)):\n",
    "        bar_width = int(prob * 100)\n",
    "        histogram_html += f\"\"\"\n",
    "        <div style='margin-right: 5px; text-align: center;'>\n",
    "            <div style='background-color: {color}; width: {bar_width}px; height: 15px;'></div>\n",
    "            <div>{label}: {prob:.2f}</div>\n",
    "        </div>\n",
    "        \"\"\"\n",
    "    histogram_html += \"</div>\"\n",
    "\n",
    "    return histogram_html\n",
    "\n",
    "# Load and sample the dataset\n",
    "def load_and_sample_data(file_path, n_dialogues_per_rating=5):\n",
    "    df = pd.read_csv(file_path)\n",
    "    df.dropna()\n",
    "\n",
    "    # Get unique ratings\n",
    "    unique_ratings = df['rating'].unique()\n",
    "\n",
    "    sampled_dialogues = []\n",
    "    for rating in unique_ratings:\n",
    "        if rating == 'nan':\n",
    "            # Skip NaN ratings\n",
    "            continue\n",
    "        # Get dialogues for this rating\n",
    "        rating_dialogues = df[df['rating'] == rating]['dialogueID'].drop_duplicates()\n",
    "        \n",
    "        # Determine how many dialogues to sample\n",
    "        n_available = len(rating_dialogues)\n",
    "        n_to_sample = min(n_dialogues_per_rating, n_available)\n",
    "        \n",
    "        # Sample dialogues, or take all if fewer than requested\n",
    "        if n_to_sample > 0:\n",
    "            sampled = rating_dialogues.sample(n=n_to_sample, replace=True)\n",
    "            sampled_dialogues.extend(sampled)\n",
    "    \n",
    "    return df[df['dialogueID'].isin(sampled_dialogues)].sort_values(['dialogueID', 'date'])\n",
    "\n",
    "def highlight_text(text, probabilities, predicted_classes):\n",
    "    tokens = tokenizer.convert_ids_to_tokens(tokenizer(text, return_tensors=\"pt\", max_length=MAX_SEQ_LENGTH).input_ids[0])\n",
    "    sentiment_map = {0: \"Very Negative\", 1: \"Negative\", 2: \"Neutral\", 3: \"Positive\", 4: \"Very Positive\"}\n",
    "    \n",
    "    highlighted_text = \"\"\n",
    "    for i, token in enumerate(tokens):\n",
    "        if token in [tokenizer.pad_token, '<|endoftext|>']:\n",
    "            continue\n",
    "        \n",
    "        sentiment_class = predicted_classes[0, i].item()\n",
    "        sentiment = sentiment_map[sentiment_class]\n",
    "        confidence_score = probabilities[0, i, sentiment_class].item()\n",
    "        \n",
    "        if sentiment in [\"Very Positive\", \"Positive\"]:\n",
    "            color = f\"rgba(0, 0, 255, {confidence_score})\"  # Blue with varying opacity\n",
    "        elif sentiment in [\"Very Negative\", \"Negative\"]:\n",
    "            color = f\"rgba(255, 165, 0, {confidence_score})\"  # Orange with varying opacity\n",
    "        else:\n",
    "            color = \"transparent\"\n",
    "        \n",
    "        highlighted_text += f'<span style=\"background-color:{color}\">{token.replace(\"Ġ\", \" \").replace(\"Ċ\", \"\")}</span>'\n",
    "    \n",
    "    return highlighted_text\n",
    "\n",
    "\n",
    "# Map the predicted classes to sentiment labels\n",
    "sentiment_map = {0: \"Very Negative\", 1: \"Negative\", 2: \"Neutral\", 3: \"Positive\", 4: \"Very Positive\"}\n",
    "def predict_rating(text):\n",
    "    # Tokenize the input text\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, max_length=512)\n",
    "    inputs = inputs.to(DEVICE)\n",
    "\n",
    "    # Set the model to evaluation mode\n",
    "    model.eval()\n",
    "\n",
    "    # Disable gradient calculations\n",
    "    with torch.no_grad():\n",
    "        # Get the model outputs\n",
    "        outputs = model(**inputs)\n",
    "        logits = outputs.logits\n",
    "\n",
    "    # Apply class weights to logits\n",
    "    weighted_logits = logits * CLASS_WEIGHTS.unsqueeze(0).unsqueeze(0)\n",
    "\n",
    "    # Convert logits to probabilities\n",
    "    probabilities = F.softmax(weighted_logits, dim=-1)\n",
    "    last_token_probs = probabilities[0, -1, :]\n",
    "\n",
    "    # Get the predicted classes for each token\n",
    "    predicted_classes = torch.argmax(weighted_logits, dim=-1)\n",
    "\n",
    "    # Get the overall sentiment (sentiment of the last token)\n",
    "    overall_sentiment_class = predicted_classes[0, -1].item()\n",
    "    overall_sentiment = sentiment_map[overall_sentiment_class]\n",
    "\n",
    "    highlighted_text = highlight_text(text, probabilities, predicted_classes)\n",
    "\n",
    "    # Add this at the end of the function\n",
    "    histogram_html = create_confidence_histogram(last_token_probs.unsqueeze(0).unsqueeze(0))\n",
    "\n",
    "    return overall_sentiment, highlighted_text, histogram_html\n",
    "\n",
    "# Process the sampled data\n",
    "def process_samples(samples):\n",
    "    results = []\n",
    "    for dialogue_id, dialogue in samples.groupby('dialogueID'):\n",
    "        dialogue_text = \"\"\n",
    "        dialogue_highlighted = \"\"\n",
    "        for _, turn in dialogue.iterrows():\n",
    "            turn_text = f\"{turn['from']}: {turn['text']}\\n\"\n",
    "            dialogue_text += turn_text\n",
    "            \n",
    "            sentiment, highlighted_text, histogram_html = predict_rating(turn_text)\n",
    "            dialogue_highlighted += f\"{highlighted_text}<br>{histogram_html}<br>\"\n",
    "\n",
    "        overall_sentiment, _, overall_histogram_html = predict_rating(dialogue_text)\n",
    "        results.append({\n",
    "            'DialogueID': dialogue_id,\n",
    "            'Dialogue': dialogue_highlighted,\n",
    "            'Predicted Rating': overall_sentiment,\n",
    "            'Overall Histogram': overall_histogram_html,\n",
    "            'GPT Labeled Rating': sentiment_map[dialogue['rating'].mode().values[0]]  # Most common rating in the dialogue\n",
    "        })\n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "# Handled pasted in conversation\n",
    "def process_conversation(conversation):\n",
    "    turns = conversation.split('\\n')\n",
    "    processed_turns = []\n",
    "    \n",
    "    for turn in turns:\n",
    "        if ':' in turn:\n",
    "            name, text = turn.split(':', 1)\n",
    "            sentiment, highlighted_text, histogram_html = predict_rating(text)\n",
    "            processed_turns.append(f\"<strong>{name.strip()}:</strong> {highlighted_text}<br>{histogram_html}<br>\")\n",
    "    \n",
    "    return \"\".join(processed_turns)\n",
    "\n",
    "# Create the Gradio interface\n",
    "def create_interface():\n",
    "    # Load and process the samples\n",
    "    samples = load_and_sample_data('data/10k-dialogues-sample-labeled.csv')\n",
    "    results = process_samples(samples)\n",
    "\n",
    "    # Create HTML table for results\n",
    "    table_html = \"<table style='width:100%; border-collapse: collapse;'>\"\n",
    "    table_html += \"<tr><th>Dialogue</th><th>Predicted Rating</th><th>GPT Labeled Rating</th></tr>\"\n",
    "    \n",
    "    for _, row in results.iterrows():\n",
    "        predicted_sentiment = row['Predicted Rating']\n",
    "        gpt_sentiment = row['GPT Labeled Rating']\n",
    "        \n",
    "        def get_sentiment_color(sentiment):\n",
    "            if sentiment in [\"Positive\", \"Very Positive\"]:\n",
    "                return \"rgba(0, 0, 255, 1)\"  # Blue\n",
    "            elif sentiment in [\"Negative\", \"Very Negative\"]:\n",
    "                return \"rgba(255, 165, 0, 1)\"  # Orange\n",
    "            else:\n",
    "                return \"transparent\"\n",
    "        \n",
    "        predicted_color = get_sentiment_color(predicted_sentiment)\n",
    "        gpt_color = get_sentiment_color(gpt_sentiment)\n",
    "        \n",
    "        table_html += f\"<tr><td style='border: 1px solid #ddd; padding: 8px; word-wrap: break-word;'>{row['Dialogue']}</td>\"\n",
    "        table_html += f\"<td style='border: 1px solid #ddd; padding: 8px; background-color: {predicted_color}; color: black;'>{predicted_sentiment}</td>\"\n",
    "        table_html += f\"<td style='border: 1px solid #ddd; padding: 8px; background-color: {gpt_color}; color: black;'>{gpt_sentiment}</td></tr>\"\n",
    "    \n",
    "    table_html += \"</table>\"\n",
    "\n",
    "    # Add the results table\n",
    "    with gr.Blocks() as demo:\n",
    "        gr.Markdown(\"# Conversation Turn Prediction\")\n",
    "        gr.Markdown(f\"\"\"\n",
    "        ## How to Read the Colors\n",
    "        - Blue: Positive sentiment (darker blue indicates higher confidence)\n",
    "        - Orange: Negative sentiment (darker orange indicates higher confidence)\n",
    "        - No color: Neutral sentiment\n",
    "        You'll see the model change its opinions as it reads the text\n",
    "        \"\"\")\n",
    "        gr.Interface(\n",
    "            fn=process_conversation,\n",
    "            inputs=gr.Textbox(lines=5, label=\"Enter conversation (format: 'name: text' for each turn)\"),\n",
    "            outputs=gr.HTML(label=\"Processed Conversation\"),\n",
    "            title=\"Conversation Turn Prediction\",\n",
    "            description=\"Enter a conversation with the format 'name: text' for each turn. The model will highlight the text based on its predicted rating.\",\n",
    "        )\n",
    "        gr.Markdown(\"## Sample Results\")\n",
    "        gr.HTML(table_html)\n",
    "\n",
    "    return demo\n",
    "\n",
    "# Launch the interface\n",
    "demo = create_interface()\n",
    "demo.launch(share=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
