{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ubuntu/ml-takehome/slingshot-venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Some weights of GPTNeoForTokenClassification were not initialized from the model checkpoint at EleutherAI/gpt-neo-125m and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/tmp/ipykernel_12021/3757544435.py:18: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load(model_weights_path, map_location=DEVICE))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GPTNeoForTokenClassification(\n",
       "  (transformer): GPTNeoModel(\n",
       "    (wte): Embedding(50257, 768)\n",
       "    (wpe): Embedding(2048, 768)\n",
       "    (drop): Dropout(p=0.0, inplace=False)\n",
       "    (h): ModuleList(\n",
       "      (0-11): 12 x GPTNeoBlock(\n",
       "        (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (attn): GPTNeoAttention(\n",
       "          (attention): GPTNeoSelfAttention(\n",
       "            (attn_dropout): Dropout(p=0.0, inplace=False)\n",
       "            (resid_dropout): Dropout(p=0.0, inplace=False)\n",
       "            (k_proj): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (v_proj): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (q_proj): Linear(in_features=768, out_features=768, bias=False)\n",
       "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
       "          )\n",
       "        )\n",
       "        (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "        (mlp): GPTNeoMLP(\n",
       "          (c_fc): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (c_proj): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (act): NewGELUActivation()\n",
       "          (dropout): Dropout(p=0.0, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (classifier): Linear(in_features=768, out_features=5, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gradio as gr\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForTokenClassification \n",
    "import torch.nn.functional as F\n",
    "import pandas as pd\n",
    "import random\n",
    "\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Load the pretrained tokenizer and our trained model\n",
    "model_checkpoint = \"EleutherAI/gpt-neo-125m\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint, add_prefix_space=True)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# Load the trained weights\n",
    "model = AutoModelForTokenClassification.from_pretrained(model_checkpoint, num_labels=5)\n",
    "model_weights_path = \"model_weights/imdb_sentiment_label_model.pth\"\n",
    "model.load_state_dict(torch.load(model_weights_path, map_location=DEVICE))\n",
    "model.to(DEVICE)  # Move the model to the appropriate device\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input text: This is a great movie! I loved it.\n",
      "Overall sentiment: Positive\n",
      "Highlighted text: <span style=\"background-color:rgba(255, 165, 0, 0.53486567735672)\"> This</span><span style=\"background-color:rgba(255, 165, 0, 0.5930820107460022)\"> is</span><span style=\"background-color:rgba(255, 165, 0, 0.5326722860336304)\"> a</span><span style=\"background-color:rgba(0, 0, 255, 0.9187238812446594)\"> great</span><span style=\"background-color:rgba(0, 0, 255, 0.986683189868927)\"> movie</span><span style=\"background-color:rgba(0, 0, 255, 0.9931069016456604)\">!</span><span style=\"background-color:rgba(0, 0, 255, 0.9904682040214539)\"> I</span><span style=\"background-color:rgba(0, 0, 255, 0.9896005988121033)\"> loved</span><span style=\"background-color:rgba(0, 0, 255, 0.9960152506828308)\"> it</span><span style=\"background-color:rgba(0, 0, 255, 0.9946004152297974)\">.</span>\n"
     ]
    }
   ],
   "source": [
    "def predict_sentiment(text):\n",
    "    # Tokenize the input text\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\", truncation=True)\n",
    "    inputs = inputs.to(DEVICE)\n",
    "\n",
    "    # Set the model to evaluation mode\n",
    "    model.eval()\n",
    "\n",
    "    # Disable gradient calculations\n",
    "    with torch.no_grad():\n",
    "        # Get the model outputs\n",
    "        outputs = model(**inputs)\n",
    "        logits = outputs.logits\n",
    "\n",
    "    # Convert logits to probabilities\n",
    "    probabilities = F.softmax(logits, dim=-1)\n",
    "\n",
    "    # Get the predicted classes for each token\n",
    "    predicted_classes = torch.argmax(logits, dim=-1)\n",
    "\n",
    "    # Map the predicted classes to sentiment labels\n",
    "    sentiment_map = {0: \"Negative\", 1: \"Mid-Negative\", 2: \"Neutral\", 3: \"Mid-Positive\", 4: \"Positive\"}\n",
    "\n",
    "    # Get the overall sentiment (mode of all token predictions)\n",
    "    # ðŸš¨\n",
    "    overall_sentiment_class = torch.mode(predicted_classes[-1]).values.item()\n",
    "    overall_sentiment = sentiment_map[overall_sentiment_class]\n",
    "\n",
    "    # Calculate overall confidence (mean of highest probabilities for each token)\n",
    "    overall_confidence = torch.mean(torch.max(probabilities[0], dim=1).values).item()\n",
    "\n",
    "    # Decode tokens and pair with their predicted classes\n",
    "    tokens = tokenizer.convert_ids_to_tokens(inputs.input_ids[0])\n",
    "    # Found this 'Ä ' character is space token, so replacing it as such\n",
    "    token_sentiments = [(i, token.replace('Ä ', ' '), predicted_classes[0, i].item()) \n",
    "                        for i, token in enumerate(tokens) \n",
    "                        if token not in [tokenizer.pad_token, '<|endoftext|>']]\n",
    "\n",
    "    # Generate highlighted text\n",
    "    highlighted_text = \"\"\n",
    "    for i, token, sentiment_class in token_sentiments:\n",
    "        sentiment = sentiment_map[sentiment_class]\n",
    "        confidence_score = probabilities[0, i, sentiment_class].item()\n",
    "        \n",
    "        if sentiment in [\"Very Positive\", \"Positive\"]:\n",
    "            color = f\"rgba(0, 0, 255, {confidence_score})\"  # Blue with varying opacity\n",
    "        elif sentiment in [\"Very Negative\", \"Negative\"]:\n",
    "            color = f\"rgba(255, 165, 0, {confidence_score})\"  # Orange with varying opacity\n",
    "        else:\n",
    "            color = \"transparent\"\n",
    "        \n",
    "        highlighted_text += f'<span style=\"background-color:{color}\">{token}</span>'\n",
    "\n",
    "    return overall_sentiment, highlighted_text\n",
    "\n",
    "# Example usage\n",
    "sample_text = \"This is a great movie! I loved it.\"\n",
    "sentiment, highlighted_text = predict_sentiment(sample_text)\n",
    "print(f\"Input text: {sample_text}\")\n",
    "print(f\"Overall sentiment: {sentiment}\")\n",
    "print(f\"Highlighted text: {highlighted_text}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7861\n",
      "* Running on public URL: https://3a16c13a98545cb3cd.gradio.live\n",
      "\n",
      "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"https://3a16c13a98545cb3cd.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load and sample the dataset\n",
    "def load_and_sample_data(file_path, n_samples=100):\n",
    "    df = pd.read_csv(file_path)\n",
    "    return df.sample(n=n_samples)\n",
    "\n",
    "# Process the sampled data\n",
    "def process_samples(samples):\n",
    "    results = []\n",
    "    for _, row in samples.iterrows():\n",
    "        sentiment, highlighted_text = predict_sentiment(row['review'])\n",
    "        results.append({\n",
    "            'Review': highlighted_text,\n",
    "            'Predicted Sentiment': sentiment,\n",
    "            'True Sentiment': row['sentiment']\n",
    "        })\n",
    "    return pd.DataFrame(results)\n",
    "\n",
    "# Create the Gradio interface\n",
    "def create_interface():\n",
    "    # Load and process the samples\n",
    "    samples = load_and_sample_data('data/IMDB Dataset.csv')\n",
    "    results = process_samples(samples)\n",
    "\n",
    "    # Create the Gradio interface\n",
    "    iface = gr.Interface(\n",
    "        fn=predict_sentiment,\n",
    "        inputs=\"text\",\n",
    "        outputs=[\"text\", \"html\"],\n",
    "        title=\"IMDB Sentiment Prediction\",\n",
    "        description=\"Enter text to predict sentiment and see highlights based on confidence.\",\n",
    "    )\n",
    "\n",
    "    # Create HTML table for results\n",
    "    table_html = \"<table style='width:100%; border-collapse: collapse;'>\"\n",
    "    table_html += \"<tr><th>Review</th><th>Predicted Sentiment</th><th>True Sentiment</th></tr>\"\n",
    "    \n",
    "    for _, row in results.iterrows():\n",
    "        predicted_sentiment = row['Predicted Sentiment']\n",
    "        if predicted_sentiment in [\"Positive\", \"Mid-Positive\"]:\n",
    "            sentiment_color = \"rgba(0, 0, 255, 1)\"  # Blue\n",
    "        elif predicted_sentiment in [\"Negative\", \"Mid-Negative\"]:\n",
    "            sentiment_color = \"rgba(255, 165, 0, 1)\"  # Orange\n",
    "        else:\n",
    "            sentiment_color = \"transparent\"\n",
    "        \n",
    "        table_html += f\"<tr><td style='border: 1px solid #ddd; padding: 8px; word-wrap: break-word;'>{row['Review']}</td>\"\n",
    "        table_html += f\"<td style='border: 1px solid #ddd; padding: 8px; background-color: {sentiment_color}; color: white;'>{predicted_sentiment}</td>\"\n",
    "        table_html += f\"<td style='border: 1px solid #ddd; padding: 8px;'>{row['True Sentiment']}</td></tr>\"\n",
    "    \n",
    "    table_html += \"</table>\"\n",
    "\n",
    "    # Add the results table\n",
    "    with gr.Blocks() as demo:\n",
    "        gr.Markdown(\"# IMDB Sentiment Prediction\")\n",
    "        # Add text field for classification metrics\n",
    "        gr.Markdown(f\"\"\"\n",
    "        ## How to Read the Colors\n",
    "        - Blue: Positive sentiment (darker blue indicates higher confidence)\n",
    "        - Orange: Negative sentiment (darker orange indicates higher confidence)\n",
    "        - No color: Neutral sentiment\n",
    "        You'll see the model change its opinions as it reads the text\n",
    "        \"\"\")\n",
    "        iface.render()\n",
    "        gr.Markdown(\"## Sample Results\")\n",
    "        gr.HTML(table_html)\n",
    "\n",
    "    return demo\n",
    "\n",
    "# Launch the interface\n",
    "demo = create_interface()\n",
    "demo.launch(share=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
